{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Dh8MkXaG-c9Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Curso de Big Data. Otoño 2024\n",
    "\n",
    "## Trabajo Práctico 1 - Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RhBlm6mZ-c9e"
   },
   "source": [
    "### Reglas de formato y presentación\n",
    "- El trabajo debe estar bien comentado (utilizando #) para que el código sea fácil de entender por sus compañeros y profesores.\n",
    "\n",
    "- El mismo debe ser completado en este Jupyter Notebook y entregado como tal, es decir en un archivo .ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEjGaa4U-c9g"
   },
   "source": [
    "### Fecha de entrega:\n",
    "Domingo 24 de marzo a las 23:59hs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N9TU2y7E-c9h"
   },
   "source": [
    "### Modalidad de entrega\n",
    "- Al finalizar el trabajo práctico deben hacer un último <i>commit</i> en su repositorio de GitHub con el mensaje “Entrega final del tp”. \n",
    "- Asegurense de haber creado una carpeta llamada TP1. Este Jupyter Notebook y el correspondiente al TP1 - Parte 2 deben estar dentro de esa carpeta.\n",
    "- También deben enviar el link de su repositorio -para que pueda ser clonado y corregido- al siguiente email: v.oubina@gmail.com\n",
    "- La última versión en el repositorio es la que será evaluada. Por lo que es importante que: \n",
    "    - No envien el correo hasta no haber terminado y estar seguros de que han hecho el <i>commit y push</i> a la versión final que quieren entregar. Debido a que se pueden tomar hasta 3 días de extensión a lo largo del curso, no se corregirán sus tareas hasta no recibir el correo.\n",
    "    - No hagan nuevos <i>push</i> despues de haber entregado su versión final. Esto generaría confusión acerca de que versión es la que quieren que se les corrija. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXbrPraa-c9i"
   },
   "source": [
    "#### Ejercicio 1\n",
    "Usando la API de Mercado Libre, obtener los ítems de una consulta de búsqueda. Pueden buscar cualquier producto de su interés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2\n",
    "Guarden los precios de los ítems obtenidos en un dataframe y calculen el precio promedio, el mínimo y el máximo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3\n",
    "Armen un histograma de los precios. ¿Ven algún <i>outlier<i>? \n",
    "Nota: pueden usar la librería de Matplotlib o la de Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4\n",
    "Realicen alguna consulta adicional utilizando la API de Mercado Libre (debe ser alguna consulta que no se haya visto en clase. Por ejemplo, obtener los ítems de un vendedor en particular, obtener los productos de una categoría u otros). Analicen los resultados y comenten uno o dos que les parezcan interesantes (por ejemplo, precios promedio de los productos de un vendedor, diferencia entre el precio original y actual, si acepta mercado pago para la compra de productos, etc.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5\n",
    "De la página de noticias del [diario La Nación](https://www.lanacion.com.ar/), utilicen herramientas de web scraping para obtener los links de las noticias de la portada. Guarden los links obtenidos en un dataframe y expórtenlo a un archivo de excel.\n",
    "\n",
    "Nota 1: es posible que logren obtener los links a las noticias sin el dominio: \"https://www.lanacion.com.ar/\". De ser así, concatenen el dominio a la ruta del link obtenido, tal que se obtenga un link al que se pueda acceder. Es decir, que las cadenas de caracteres finales tendrán la forma: https://www.lanacion.com.ar/*texto_obtenido*)\n",
    "\n",
    "Nota 2: junto con su entrega, adjunten una captura de la página de noticias al momento de correr su código. Eso servirá al momento de la corrección para verificar que los links obtenidos hacen referencia a las noticias de ese día y hora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: bs4 in /opt/anaconda3/lib/python3.11/site-packages (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4->bs4) (2.5)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: openpyxl in /opt/anaconda3/lib/python3.11/site-packages (3.0.10)\n",
      "Requirement already satisfied: et_xmlfile in /opt/anaconda3/lib/python3.11/site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "#En primer lugar, instalaremos las librerías para luego poder importarlas. Para poder hacer web scrapping necesitamos acceder al código \n",
    "#HTML; para esta tarea utilizaremos la librería requests de Python. Instalaremos Beautiful Soup, Pandas para poder trabajar con data frames y \n",
    "# openpyxl\n",
    " \n",
    "!pip install requests\n",
    "!pip install bs4\n",
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "\n",
    "#Luego, las importamos.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En primer lugar voy a hacer la solicitud a la página web.\n",
    "url = 'https://www.lanacion.com.ar'\n",
    "\n",
    "#Con el método get de la librería requests traigo el contenido de HTML. El método get obtiene el contenido de una página web a \n",
    "# de una solicitud HTTP GET y nos permite trabajar con la response obtenida en nuestro código de Python.\n",
    "response = requests.get(url)\n",
    "\n",
    "#Una vez que hicimos el request los podemos ver con el método content; nos permite acceder a los datos en su forma original de bytes.\n",
    "contenido = response.content\n",
    "\n",
    "# Creo una variable \"soup\" donde usamos Beautiful Soup que toma como primer parámetro el contenido HTML que queremos analizar y como\n",
    "#segundo parámetro 'html.parser' que especifica que queremos utilizar el analizador HTML incorporado de Python para analizar el \n",
    "# contenido HTML.\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para encontrar los enlaces de las noticias en la portada creamos una lista nueva llamada news_links.\n",
    "news_links = []\n",
    "\n",
    "#Utilizamos la función find_all que devuelve una lista de todos los elementos '<a>' (enlaces) dentro de soup.\n",
    "elementos_a = soup.find_all('a')\n",
    "\n",
    "#Con for recorremos link por link y finalmente le pedimos que guarde (utilizando el método append) en la lista news_links a los que\n",
    "#empiezen con https://www.lanacion.com.ar; es decir, los links pertenecientes a la página web de La Nación y que cumplan con que se\n",
    "# hayan recopilado todos los enlaces de la página web en forma de URL.\n",
    "for link in elementos_a:\n",
    "    href = link.get('href')\n",
    "    if href and href.startswith('https://www.lanacion.com.ar'):\n",
    "        news_links.append(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.lanacion.com.ar/tema/javier-milei-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.lanacion.com.ar/dolar-hoy/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.lanacion.com.ar/tema/seleccion-arg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.lanacion.com.ar/dolar-hoy/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.lanacion.com.ar/tema/dolar-blue-ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>https://www.lanacion.com.ar/revista-jardin/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>https://www.lanacion.com.ar/revista-lugares/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>https://www.lanacion.com.ar/mapa-del-sitio/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>https://www.lanacion.com.ar/arc/outboundfeeds/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>https://www.lanacion.com.ar/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Links\n",
       "0    https://www.lanacion.com.ar/tema/javier-milei-...\n",
       "1               https://www.lanacion.com.ar/dolar-hoy/\n",
       "2    https://www.lanacion.com.ar/tema/seleccion-arg...\n",
       "3               https://www.lanacion.com.ar/dolar-hoy/\n",
       "4    https://www.lanacion.com.ar/tema/dolar-blue-ti...\n",
       "..                                                 ...\n",
       "99         https://www.lanacion.com.ar/revista-jardin/\n",
       "100       https://www.lanacion.com.ar/revista-lugares/\n",
       "101        https://www.lanacion.com.ar/mapa-del-sitio/\n",
       "102  https://www.lanacion.com.ar/arc/outboundfeeds/...\n",
       "103                       https://www.lanacion.com.ar/\n",
       "\n",
       "[104 rows x 1 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ahora crearemos un DataFrame con los enlaces de las noticias.\n",
    "df = pd.DataFrame({'Links': news_links})\n",
    "(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Los enlaces de las noticias se han guardado en 'noticias_lanacion.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Luego, exportaremos el DataFrame a un archivo Excel con el método .to_excel especificando en el primer parámetro el nombre del \n",
    "# archivo de Excel en el que se exportará el Data Frame y como segundo parámetro ponemos index=False para indicar que no vamos a \n",
    "#incluir el índice DataFrame en el archivo de Excel.\n",
    "df_2 = df.to_excel('noticias_lanacion.xlsx', index=False)\n",
    "print(df_2)\n",
    "\n",
    "print(\"Los enlaces de las noticias se han guardado en 'noticias_lanacion.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "TP1 - Parte 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
